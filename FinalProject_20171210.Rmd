---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Cleaning

This dataset was downloaded from https://www.kaggle.com/zynicide/wine-reviews on 11/28/2017. It was scraped from WineEnthusiast on November 22nd, 2017

### Identifying the Vintage Year

```{r, warning=FALSE}

##Load packages
library(stringr)
library(dplyr)

##Read in the dataset
wine <- read.csv("winemag-data-130k-v2.csv")

##Identify the wine vintage from the wine title
pattern <- "\\d\\d\\d\\d" #for the 4 digits that represent year
x <- str_detect(wine$title, pattern) #just for my own practice so I could look at x and find out how much missing data there were 
length(wine$X) - sum(x) #4609 wines lack year
#if you look in the environment window under this dataset, it shows title as a factor with 118840 levels -- I am not sure where that number 118840 comes from, as each of these should be unique, right?

wine$year <- str_extract(wine$title, pattern) ##Amanda note: by adding the "wine$.." in front it joins the year column to the wine dataset, saving you the work you had below of left joining

##look at the fields for wines that are missing year to make sure there isn't other 
##information we could use
missing <- wine %>% filter(str_detect(title, pattern)!=1)
# head(missing, n=100)
####I didn't notice any years here, but I did notice they were mostly 
####sparkling/champange wines that were missing year, which we will probably want to ####exclude anyway

##Are there any values that seem unusual?
wine %>% filter(year<1990 | year>2017)
####From the titles I noticed that sometimes they sometimes have other numbers which 
####seem to often be the year the winery opened or sometmes are just part of the name 
####of the wine

##It seems like only years from 1985(ish)-2017 are actually vintage years. Here's a ##second try at identifying the year based on this insight.
validyears <- as.character(seq(1985, 2017, by=1))
wine$year <- str_extract(wine$title, paste(validyears, collapse="|"))
# table(wine$year)
wine %>% filter(is.na(wine$year))
###There are 4,662 missing vintage

```

### Geocoding the wine region

We will need to know the region in terms of longitude and latitude in order to map this information and link to external data sources with weather information. The geonames package seems to be the best free datasource out there with this type of information.

```{r}

##Installing any packages needed for geocoding
if(!require(mapproj)){
    install.packages("mapproj")
    library(mapproj)
}

if(!require(ggmap)){
    install.packages("ggmap")
    library(ggmap)
}

if(!require(rworldmap)){
    install.packages("rworldmap")
    library(rworldmap)
}

if(!require(sp)){
    install.packages("sp")
    library(sp)
}

if(!require(countrycode)){
    install.packages("countrycode")
    library(countrycode)
}

if(!require(geonames)){
    install.packages("geonames")
    library(geonames)
}

##from the wines dataset, select unique values of the region variable

regions <- as.data.frame(as.character(unique(wine$region_1)))
colnames(regions) <- c("region_1")
regions <- regions %>% filter(is.na(region_1)==FALSE & region_1!="")
#nrow(regions)

##Geocode the regions
##NOTE: Don't run this code unless you need to recreate the table of coordinates. Google have limits for the number of queries. I ran this once and saved it as an R file
# empty = as.data.frame(matrix(rep(NA,3), ncol = 3))
# colnames(empty) = c("longitude", "latitude", "address")
# 
# geo = sapply(1:nrow(regions), function(x){
#   charregion <- as.character(regions[x,])
#   geo_result = tryCatch(geocode(charregion, output = 'latlona'), 
#            error = function(e) {empty}) 
#  if (length(geo_result) !=3)
#  {
#    return(empty)
#  }
#    else(return(geo_result))
#   }) 
# 
# geo = t(geo) %>% as.data.frame() %>% mutate(region_1 = regions$region_1)
# 
# save(geo, file="geo.RData")

load(file="geo.RData")

##Now check whether this geocoding worked by seeing if these addresses match the country in the wines dataset

## Step 1: Join those latitude and longitude coordinates to a country ID (function sourced from https://stackoverflow.com/questions/14334970/convert-latitude-and-longitude-coordinates-to-country-name-in-r)

coords2country = function(points)
{  
  countriesSP <- getMap(resolution='low')

  #setting CRS directly to that from rworldmap
  pointsSP <- SpatialPoints(points, proj4string=CRS(proj4string(countriesSP)))  

  # use 'over' to get indices of the Polygons object containing each point 
  indices <- over(pointsSP, countriesSP)

  # return the ISO3 names of each country
  indices$ISO3
}

##Feed this function a dataset called "points" with the long and lat as the 2 
##columns

geo <- geo %>% filter(is.na(longitude)==FALSE) %>% ##filter unmatched rows
  mutate(longitude=as.numeric(longitude), latitude=as.numeric(latitude))
  
points <- geo %>%  select(longitude, latitude)

geo$countryISO3 <- coords2country(points)

##Step 2: get a country code for the rows in the wines dataset

##Set options for the geonames package
###Set username (I created a profile on the website http://www.geonames.org/)
options(geonamesUsername="bi0260finalproject")
###Set api
options(geonamesHost="api.geonames.org")
###check that connection is working (some samples should pop up)
#source(system.file("tests","testing.R",package="geonames"),echo=TRUE)


##Add a column with the three character country code
wine$countryISO3 <- countrycode(ifelse(wine$country=="England", "United Kingdom", as.character(wine$country)), 'country.name', 'iso3c') 

##Step 3: Join on the lat and long coordinates by region name and check that the countries match

wine_v2 <- left_join(wine, geo, by="region_1")

unmatched <- wine_v2 %>% 
  filter(is.na(longitude)==TRUE | countryISO3.x!=countryISO3.y)

nrow(wine_v2)
nrow(unmatched)

##Fix the latitude and longitude fields to be NA when the countries don't match
wine_v2 <- wine_v2 %>% 
  mutate(longitude=ifelse(countryISO3.x==countryISO3.y, longitude, NA)) %>%
  mutate(latitude=ifelse(countryISO3.x==countryISO3.y, latitude, NA))

##Now for all the unmatched, try to match to coordinates based on province instead

provinces <- as.data.frame(as.character(unique(unmatched$province)))
colnames(provinces) <- c("province")
provinces <- provinces %>% filter(is.na(province)==FALSE & province!="")
nrow(provinces)

##Geocode the regions
##NOTE: Don't run this code unless you need to recreate the table of coordinates. Google have limits for the number of queries. We ran this once and saved it as an R file
# empty = as.data.frame(matrix(rep(NA,3), ncol = 3))
# colnames(empty) = c("longitude", "latitude", "address")
# 
# geo2 = sapply(1:nrow(provinces), function(x){
#   charprovince <- as.character(provinces[x,])
#   geo_result = tryCatch(geocode(charprovince, output = 'latlona'), 
#            error = function(e) {empty}) 
#  if (length(geo_result) !=3)
#  {
#    return(empty)
#  }
#    else(return(geo_result))
#   }) 
# 
# geo2 = t(geo2) %>% as.data.frame() %>% mutate(province = provinces$province)
# 
# save(geo2, file="geo2.RData")

load(file="geo2.RData")

##Now check whether this geocoding worked by seeing if these addresses match the country in the wines dataset

## Step 1: Join those latitude and longitude coordinates to a country ID function 
##Feed the function above into a dataset called "points" with the long and lat as the 2 columns

geo2 <- geo2 %>% filter(is.na(lon)==FALSE) %>% ##filter unmatched rows
  mutate(longitude=as.numeric(lon), latitude=as.numeric(lat))
  
points2 <- geo2 %>%  select(longitude, latitude)

geo2$countryISO3.z <- coords2country(points2)
 head(geo2)

##Step 2: Join on the lat and long coordinates by province name and check that the countries match

wine_v3 <- left_join(wine_v2, geo2, by="province")

unmatched2 <- wine_v3 %>% 
  mutate(longitude=ifelse(is.na(longitude.x)==FALSE,longitude.x, longitude.y )) %>%
  mutate(latitude=ifelse(is.na(latitude.x)==FALSE,latitude.x, latitude.y )) %>%
  filter(is.na(longitude)==TRUE | countryISO3.x!=countryISO3.z)

# nrow(wine_v3)
# nrow(unmatched2)
##We ssucessfully matched 71% 

wine_v3 <- wine_v3 %>% 
  mutate(longitude=ifelse(is.na(longitude.x)==TRUE &
                            countryISO3.x==countryISO3.z,
                          longitude.y, 
                          longitude.x)) %>%
  mutate(latitude=ifelse(is.na(latitude.x)==TRUE &
                            countryISO3.x==countryISO3.z,
                          latitude.y, 
                          latitude.x)) %>%
  select(X, country, designation, points, price, province, region_1, region_2, taster_name, title, variety, winery, year, countryISO3.x, longitude, latitude)

```


### Linking to Climate Data

We used the National Ocean and Atmospheric Association's climate data (https://www.ncdc.noaa.gov/cdo-web/webservices/v2). There is an R package called rnoaa which is a client for this dataset. 


```{r}
if(!require(rnoaa)){
    install.packages("rnoaa")
    library(rnoaa)
}
station_data <- ghcnd_stations()

#install pbapply to add a progress bar to the *apply functions and check their progress (very important when code take many hours to run)
if(!require(pbapply)){
    install.packages("pbapply")
    library(pbapply)
}

##Restrict to just red wines and ones with coordinates (this helped the dataset be a somewhat more manageable size)

redwines <- c("Zinfandel", "Merlot", "Malbec", "Syrah", "Shiraz", "Sangiovese", "Cabernet Sauvignon", "Pinot Noir", "Barbera", "Aglianico", "Tinto Fino", "Tinta de Toro", "Tempranillo Blend", "Tempranillo", "Tannat", "Sangiovese", "Sangiovese Grosso", "Red Blend", "RhÃ´ne-style Red Blend", "Primitivo", "Portuguese Red", "Petite Sirah", "Petit Verdot", "Nero d'Avola", "Nerello Mascalese", "Nebbiolo", "MourvÃ¨dre", "Monastrell", "Meritage", "MencÃa", "Malbec-Merlot", "Grenache", "Garnacha", "Gamay", "Corvina, Rondinella, Molinara", "CarmenÃ¨re", "Cabernet Franc", "Bordeaux-style Red Blend", "Bonarda", "Barbera", "Aglianico")

wine_v4 <- wine_v3 %>% filter(is.na(longitude) == FALSE & variety %in% redwines)

#Obtain the closest weather station id based on latitude and longitude coordinates
##NOTE: Don't run this code unless you need to recreate the tables. We ran this code in batches and then rbind'ed them together. In total, this woudl take about 5-10 hours depending on your connection speed.
# 
# lat_lon_dfinput <- wine_v4 %>% mutate(id=X) %>% select(id, latitude, longitude)
# 
# 
# stationids <-  pbsapply(1:nrow(lat_lon_dfinput), function(x) {
#   output <- meteo_nearby_stations(lat_lon_df=lat_lon_dfinput[x,], 
#                           station_data = station_data, limit = 1)
#   output[[1]][1]
# }) %>% t() 
# 
# wine_v5 <- wine_v4 %>% 
#   mutate(stationid = as.character(stationids[1,])) %>%
#   select(X, stationid)
# 
# ##inner join back to the wine_v4
# 
# wine_v6 <- inner_join(wine_v4, wine_v5, by="X")
# 
# #getting min and max dates for growing seasons based on hemispheres
# #northern hemisphere: aug-oct harvest season, southern hemisphere: feb-april harvest season
# #website source:  http://winefolly.com/tutorial/start-planning-now-wine-harvest-season/
# minmonth <- ifelse(wine_v6$latitude>0,"08","02")
# wine_v6$min <- as.Date(paste(wine_v6$year, minmonth, "01", sep="-"), "%Y-%m-%d")
# maxmonth <- ifelse(wine_v6$latitude>0,"10","04")
# maxday <- ifelse(wine_v6$latitude>0, "31", "30")
# wine_v6$max <- as.Date(paste(wine_v6$year, maxmonth, maxday ,sep="-"), "%Y-%m-%d")
# 
# #Obtain the average precipitation and temperature for this date range and weather station combination.
# #precip is in tenths of mm, tempavg is in tenths of degrees C
##NOTE: Don't run this code unless you need to recreate the tables. We ran this code in batches and then rbind'ed them together. In total, this would take about 20 hours- 1.5 days depending on your connection speed.
# 
# empty = matrix(rep(999,2), nrow = 1)
# f = pbsapply(1:nrow(wine_v6), function(x) {
#   tryCatch(meteo_tidy_ghcnd(as.character(wine_v6$stationid[x]), 
#                      keep_flags = FALSE, 
#                      var = c("prcp","tavg"), 
#                      date_min = as.character(wine_v6$min[x]), 
#                      date_max = as.character(wine_v6$max[x])) %>% 
#     # group_by(id) %>%
#     summarize(avgprcp=mean(prcp/10, na.rm=TRUE),
#             avgtemp=mean(tavg/10, na.rm=TRUE)), error = function(e) {empty})
#   }) %>% t() %>% as.data.frame()
# 
# wine_v7 <- wine_v6 %>% 
#   mutate(avgprcp = as.numeric(f$V1), avgtemp = as.numeric(f$V2), year = as.numeric(year))
#
# save(wine_v7, file="wine_v7.RData")

load(file="wine_v7.RData")

```


## Data Analysis

Data Analysis for this project included descriptive analyses, data visualizations (including maps), and machine learning.

###Descriptive Analysis

Descriptive analyses include histograms for wine ratings, price, and years in our dataset

```{r}

if(!require(ggthemes)){
    install.packages("ggthemes")
    library(ggthemes)
}

##histogram for wine ratings
p <- wine_v4 %>% 
  ggplot(aes(points)) + 
  geom_histogram(binwidth = 1) + 
  labs(y= "Count", x="Expert Wine Ratings (100 Point Scale)",title="Distribution of Expert Wine Ratings") +
  theme_economist()

# jpeg(filename="hist_ratings.jpg")
p
# dev.off()

summary(wine_v4$points)
sd(wine_v4$points)

##histogram for wine prices
p <- wine_v4 %>% 
  ggplot(aes(price)) + 
  geom_histogram(binwidth = 0.5) + 
  labs(y= "Count", x="Wine Price ($ per Bottle)",title="Distribution of Wine Prices") +
  theme_economist()

# jpeg(filename="hist_prices.jpg", width = 710, height = 480)
  p + scale_x_continuous(trans = "sqrt", breaks = c(0, 25, 100, 400, 900, 1600, 2500))
# dev.off()

summary(wine_v4$price)

##histogram for years
p <- wine_v4 %>% 
  ggplot(aes(as.numeric(year))) + 
  geom_histogram(binwidth = 1) + 
  labs(y= "Count", x="Vintage Year",title="Distribution of Vintage Years") +
  theme_economist()

# jpeg(filename="hist_vintage.jpg", width = 710, height = 480)
  p + scale_x_continuous(breaks = c(1985, 1990, 1995, 2000, 2005, 2010, 2015))
# dev.off()


summary(as.numeric(wine_v4$year))

```

###Data Visualization

Data visualizations include average rating by price (correlation plot), average ratings by type of wine (using boxplots), average ratings by country (using maps), average ratings by country and year (using the tile plots in R), average ratings by type of wine and year (using the tile plots in R), and average rating by precipitation and temperature (correlation plots).

```{r}

##average ratings by wine type

p <- wine_v4 %>% 
  mutate(variety = reorder(variety, -points, FUN = median)) %>%
  ggplot(aes(variety, points)) +
  geom_boxplot()  +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x=" ",title="Do Some Wine Varieties Receive Better Expert Ratings?") +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# jpeg(filename="rating_byvariety.jpg", width = 710, height = 480)
  p
# dev.off()


##rating by price correlaton
##break this into categories of prices because otherwise the graph is too clumped to see correlations. We can also see from this that price makes a bigger difference in terms of ratings at lower prices and then no difference past $500
p <- wine_v4 %>% 
  filter(is.na(price)==FALSE) %>%
  mutate(pricecat = ifelse(price<100, "< $100 Bottles", ifelse(price<500, "$100-$499.99 Bottles", "\u2265 $500 Bottles"))) %>%
  mutate(pricecat = reorder(pricecat, price, FUN = median)) %>%
  ggplot(aes(price, points)) +
  geom_point()  +
  facet_grid(. ~ pricecat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Wine Price ($ per Bottle)",title="Are Price and Expert Rating Correlated?") +
  theme_economist()

# jpeg(filename="rating_byprice.jpg", width = 710, height = 480)
 p
# dev.off()

##average ratings by country
##Use the rworldmap package
if(!require(rworldmap)){
    install.packages("rworldmap")
    library(rworldmap)
}

###first, aggregate ratings data by country
dat <- wine_v4 %>% 
  filter(is.na(points)==FALSE) %>%
  group_by(countryISO3.x, country) %>%
  summarize(meanrating = mean(points), meanprice = mean(price))

dat %>% arrange(-meanrating) %>% select(country,countryISO3.x, meanrating)

###then, map these data
sPDF <- joinCountryData2Map(dat,
                            joinCode = "ISO3",
                            nameJoinColumn = "countryISO3.x")

mapDevice() #create world map shaped window


##across the world

# jpeg(filename="rating_bycountry.jpg", width = 710, height = 480)
mapCountryData(sPDF,
               nameColumnToPlot='meanrating',
               mapTitle='Mean Expert Wine Rating by Country')
# dev.off()

##in Europe

# jpeg(filename="rating_byEurope.jpg", width = 710, height = 480)
mapCountryData(sPDF,
               nameColumnToPlot='meanrating',
               mapTitle='Mean Expert Wine Rating by Country in Europe',
               mapRegion="europe")
# dev.off()

##Mean Prices by country (not shown on website)
##across the world
mapCountryData(sPDF,
               nameColumnToPlot='meanprice',
               mapTitle='Mean Wine Price by Country')

##in Europe
mapCountryData(sPDF,
               nameColumnToPlot='meanprice',
               mapTitle='Mean Wine Price by Country in Europe',
               mapRegion="europe")

##average ratings by country and year
if(!require(RColorBrewer)){
    install.packages("RColorBrewer")
    library(RColorBrewer)
}

dat2 <- wine_v4 %>% 
  filter(is.na(points)==FALSE & is.na(year)==FALSE) %>%
  group_by(country, year) %>%
  summarize(meanrating = mean(points), total.count=n()) %>%
  ungroup()

breaks <- seq(80, 94, by=2)
dat2$rating_rescale <- cut(dat2$meanrating,breaks = breaks)
dat2$rating_rescale <- factor(as.character(dat2$rating_rescale),
levels=rev(levels(dat2$rating_rescale)))


p <- dat2 %>% mutate(country = reorder(country, total.count, FUN = max)) %>%
  ggplot(aes(year, country)) + geom_tile(aes(fill = rating_rescale), colour = "white",size=0.25) + 
  labs(x="Year",y="",title="Have Country-Specific Wine Preferences Changed Over Time?") +
  scale_y_discrete(expand=c(0,0)) +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="right", legend.justification = c("left", "top"), legend.title = element_text(size=14)) +
  scale_fill_manual(values=rev(brewer.pal(7,"YlOrRd")),na.value="grey90", name="Mean Expert Rating") +
  guides(fill=guide_legend(ncol=1,bycol=TRUE))

# jpeg(filename="ratings_bycountryyear.jpg", width = 710, height = 480)
p
# dev.off()

##average ratings by variety and year

dat3 <- wine_v4 %>% 
  filter(is.na(points)==FALSE & is.na(variety)==FALSE & is.na(year)==FALSE) %>%
  group_by(variety, year) %>%
  summarize(meanrating = mean(points), total.count=n()) %>%
  ungroup()

breaks <- seq(80, 94, by=2)
dat3$rating_rescale <- cut(dat3$meanrating,breaks = breaks)
dat3$rating_rescale <- factor(as.character(dat3$rating_rescale),
levels=rev(levels(dat3$rating_rescale)))


p <- dat3 %>% mutate(variety = reorder(variety, meanrating, FUN = median)) %>%
  ggplot(aes(year, variety)) + geom_tile(aes(fill = rating_rescale), colour = "white",size=0.25) + 
  labs(x="Year",y="",title="Have Wine Variety Preferences Changed Over Time?") +
  scale_y_discrete(expand=c(0,0)) +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="right", legend.justification = c("left", "top"), legend.title = element_text(size=14)) +
  scale_fill_manual(values=rev(brewer.pal(7,"YlOrRd")),na.value="grey90", name="Mean Expert Rating") +
  guides(fill=guide_legend(ncol=1,bycol=TRUE))

# jpeg(filename="ratings_byvarietyyear.jpg", width = 710, height = 480)
p
# dev.off()

##Correlation between average precipitation and wine rating
p <- wine_v7 %>% 
  filter(avgprcp!=999) %>%
  mutate(raincat = ifelse(avgprcp<10, "< 10 mm", "\u2265 10 mm")) %>%
  mutate(raincat = reorder(raincat, avgprcp, FUN = median)) %>%
  ggplot(aes(avgprcp, points)) +
  geom_point()  +
  facet_grid(. ~ raincat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Average Daily Rainfall During Growing Season (mm)",title="Are Rainfall and Expert Rating Correlated?") +
  theme_economist()

 # jpeg(filename="rating_byrain.jpg", width = 710, height = 480)
 p
 # dev.off()
 
 ##Correlation between average temperature and wine rating
p <- wine_v7 %>% 
  filter(avgtemp!=999) %>%
  mutate(tempcat = ifelse(avgtemp<13, "< 13 (°C)", "\u2265 13 (°C)")) %>%
  mutate(tempcat = reorder(tempcat, avgtemp, FUN = median)) %>%
  ggplot(aes(avgtemp, points)) +
  geom_point()  +
  facet_grid(. ~ tempcat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Average Temperature During Growing Season (°C)",title="Are Temperature and Expert Rating Correlated?") +
  theme_economist()

 # jpeg(filename="rating_bytemp.jpg", width = 710, height = 480)
 p
 # dev.off()
 
```

###Machine Learning 

We predicted wine ratings based on the other characteristics available in our dataset (including price, longitude, latitude, wine variety, year, precipitation, and temperature) using random forests.

```{r}
if(!require(randomForest)){
    install.packages("randomForest")
    library(randomForest)
}

if(!require(caret)){
    install.packages("caret")
    library(caret)
}

set.seed(658)

##Create a dataset with no missing information
##also, reset wine variety as factor because it current recognizes >700 levels of variety from the original dataset when there are now only 35 in our final dataset
wine_v7_ml <- wine_v7 %>% 
  filter(avgtemp!=999 & 
           avgprcp!=999 & 
           is.na(points)==FALSE & 
           is.na(price)==FALSE & 
           is.na(longitude)==FALSE &
           is.na(variety)==FALSE &
           is.na(year)==FALSE) %>%
  mutate(variety = as.factor(as.character(variety)))

##rows in final dataset
nrow(wine_v7_ml)
         
##Step 1: run a basic random forest to get started

fit <- randomForest(points ~ price + longitude + latitude  + variety +
                      year + avgtemp + avgprcp,
                      data=wine_v7_ml, 
                      importance=TRUE, 
                      ntree=500)

fit

##Step 2: decide how many trees need to be run
##plot fit to see how it improves with each tree
plot(fit)

##identify the minimum mse by number of trees
which.min(fit$mse)
  ##although we don't get that much return by fitting more trees, the minimum mse is at 488 trees so we will fit that many in analyses below.

##Step 3: tune the "mtry" variable which defines how many features are randomly selected at each split. By default, it uses 2 (we can see this in the output of "fit" and the default for numeric responses is max(floor(ncol(x)/3))
##NOTE: This takes about 20 minutes so don't need to re-run

# ctrl <- trainControl(method="repeatedcv", 
#                      number=10, 
#                      repeats=10,
#                      verboseIter = TRUE)
# 
# grid_rf <-expand.grid(.mtry = c(1, 2, 4, 6))
# 
# m_rf <- train(points ~ price + longitude + latitude  + variety +
#                       year + avgtemp + avgprcp,
#               data=wine_v7_ml, method="rf", 
#               metric="RMSE", trcontrol=ctrl, 
#               tuneGrid=grid_rf)
# 
# m_rf

#Re-fit the random forest with mtry=6
fit <- randomForest(points ~ price + longitude + latitude  + variety +
                      year + avgtemp + avgprcp,
                    data=wine_v7_ml, 
                    importance=TRUE, 
                    ntree=500,
                    mtry=6)

fit

#Also, check that this RMSE is similar (for mtry=2) to that output from the model. In general, random forest is less susceptible to overfitting so cross-validation is not always necessary.


##plot the variable importance
# jpeg(filename="predvsactual.jpg", width = 710, height = 480)
varImpPlot(fit,
           sort = T,
           main="Variable Importance")


##Plot the actual vs predicted values (a visual way of looking at the error)
pred.forest <- predict(fit,wine_v7_ml)

p <- wine_v7_ml %>% 
  ggplot(aes(points, pred.forest)) +
  geom_point()  +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Predicted Ratings", x="Actual Ratings",title="Predicted vs. Actual Ratings Using Random Forest") +
  theme_economist()

 jpeg(filename="predvsactual.jpg", width = 426, height = 288)
 p
 dev.off()
 
 fit


```





