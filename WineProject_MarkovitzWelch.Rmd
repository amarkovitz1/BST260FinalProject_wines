---
title: "Can Machine Learning Replace Wine Raters?"
author:  "Amanda Markovitz and Elizabeth Welch"
date:  "December 11, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Motivation

Being comfortable ordering a fine bottle of wine at a business lunch or selecting an appropriate bottle of wine as a gift is both a skill and an art form.  It is nuanced – one needs enough savvy and sophistication to prevent social awkwardness but not so much that there is an air of pretension.  And, everyone is always in search of that elusive wine that is a great deal – high quality for its price.  This selection process very often relies on the perspectives of “experts”.  However, are professional tasters/raters really worth their esteem and their salaries?  Or, could the arguably “best” wines actually be predicted from algorithms built using publicly accessible data?

**Inspiration**

1. Our curiosity was initially piqued by reading about [Vivino](https://www.vivino.com/app), an app which targets ordinary people and guides them through choosing wines with a few finger taps.  Vivino’s rapidly growing database (of over 3 million wines based on the contributions of over 6 million) attests to its filling a much-needed niche.

2. The various class lectures on machine learning gave us a glimpse of its power as a predictive tool and its potential as an ultimate replacement for many types of routine human work. 

**Project Goal**

Our primary aim was to get hands-on practice dealing with true “big data” – large in dimension, untidy in format, with missingness, and eventually needing to be merged with one or more other data sets in order to answer a given scientific question.  As such, we chose something that we knew would be conceptually interesting and yet intellectually challenging.

**Research Questions**

1. What factors are most significantly associated with wine quality (with "quality" defined based on expert rating)?

2. Are certain climate trends (extreme temperatures and considerable rainfall) during the wine’s harvesting season important determinants of wine quality?

3. Do you "get what you pay for" when you buy an expensive bottle of wine?

4. Can a machine learning algorithm be built that could predict wine quality as effectively as expert tasters/raters?   

This last question evolved over the past couple of weeks.  At the outset, we intended to use a data set involving non-experts' ratings in order to do our predictive machine learning;  we had to shift our plan when we realized that our identified data set was not publicly accessible.

##Data

**Sources of Data**

We collected our main data set from [Kaggle](https://www.kaggle.com/zynicide/wine-reviews), which included information on 129,971 wines.  Notably, the Kaggle user had recently scraped from Wine Enthusiast.  The outcome variable was points (i.e. expert’s rating, on a scale from 0 to 100).  Important predictor variables included some geographic ones (country, province, and up to two regions), price, title (in which vintage year was embedded), and variety.  

Additionally, since weather can influence wine quality, we gathered climate data from the [National Centers for Environmental Information National Ocean and Atmospheric Association](https://www.ncdc.noaa.gov/IPS/mcdw/mcdw.html).  Specifically, we found records of daily mean temperature and daily precipitation for weather stations around the world.

**Data Scraping and Cleaning**

The first step entailed getting vintage year for each wine, and we data scraped from the title column of the original data set, extracting numbers from the strings.  There were 4,662 wines for which we could not get this information;  these years were entered as NAs.
 
Next, we needed the latitude and longitude coordinates for the region (or, when that could not be matched to coordinates, for the province) containing the vineyard where each wine was made.  This geocoding was done using R’s geonames package.  We verified the accuracy of this process by checking that the country codes produced by geocoding corresponded to the countries in the data set.  The match rate was 71%.

We proceeded to find the closest global weather station (named “stationID”) associated with each latitude, longitude coordinate pair.  This was done using R’s rnoaa package.

At this point, we filtered our data set to include only red wines that also had coordinates/station IDs.  The resultant subset had 53,126 wines.

Last, we scraped from the climate data set to pull the daily mean temperature and daily precipitation measures.  Based on our [background reading](http://winefolly.com/tutorial/start-planning-now-wine-harvest-season/), we learned that the harvesting season for vineyards in the northern hemisphere runs from August through October and for those in the southern hemisphere from February through April.  After classifying each wine according to its hemisphere, we took averages over all recorded days of mean temperature and total rainfall within the appropriate three-month period in each wine's particular vintage year.  For 69% of wines, climate information was not available, in which case the indices were entered as 999s.  

Upon filtering to rows with no missing values, our final, cleaned-up data set had 16,315 red wines.  This was the data set used for machine learning analyses.  Descriptive analyses and data visualizations were performed using the data set of 53,126 red wines.

### Identifying the Vintage Year

```{r, warning=FALSE, message=FALSE}

##Load packages
library(stringr)
library(dplyr)

##Read in the dataset
wine <- read.csv("winemag-data-130k-v2.csv")

##Identify the wine vintage from the wine title
pattern <- "\\d\\d\\d\\d" #for the 4 digits that represent year
x <- str_detect(wine$title, pattern) #to look at x and find out how much missing data there were 
length(wine$X) - sum(x) #4609 wines lack year
wine$year <- str_extract(wine$title, pattern)

##look at the fields for wines that are missing year to make sure there isn't other information we could use
missing <- wine %>% filter(str_detect(title, pattern)!=1)
# head(missing, n=100)
##I didn't notice any years here, but I did notice they were mostly sparkling/champagne wines that were missing year, which we will probably want to exclude anyway

##Are there any values that seem unusual?
# wine %>% filter(year<1990 | year>2017)
##From the titles I noticed that sometimes they have other numbers which seem to often be the year the winery opened or sometimes are just part of the name of the wine

##It seems like only years from 1985(ish)-2017 are actually vintage years. Here's a second try at identifying the year based on this insight.
validyears <- as.character(seq(1985, 2017, by=1))
wine$year <- str_extract(wine$title, paste(validyears, collapse="|"))
# table(wine$year)
# wine %>% filter(is.na(wine$year))
##There are 4,662 missing vintage

```

### Geocoding the wine region

We will need to know the region in terms of longitude and latitude in order to map this information and link to external data sources with weather information. The geonames package seems to be the best free datasource out there with this type of information.

```{r, warning=FALSE, message=FALSE}

##Installing any packages needed for geocoding
if(!require(mapproj)){
    install.packages("mapproj")
    library(mapproj)
}

if(!require(ggmap)){
    install.packages("ggmap")
    library(ggmap)
}

if(!require(rworldmap)){
    install.packages("rworldmap")
    library(rworldmap)
}

if(!require(sp)){
    install.packages("sp")
    library(sp)
}

if(!require(countrycode)){
    install.packages("countrycode")
    library(countrycode)
}

if(!require(geonames)){
    install.packages("geonames")
    library(geonames)
}

##from the wines dataset, select unique values of the region variable

regions <- as.data.frame(as.character(unique(wine$region_1)))
colnames(regions) <- c("region_1")
regions <- regions %>% filter(is.na(region_1)==FALSE & region_1!="")
#nrow(regions)

##Geocode the regions
##NOTE: Don't run this code unless you need to recreate the table of coordinates. Google have limits for the number of queries. I ran this once and saved it as an R file
# empty = as.data.frame(matrix(rep(NA,3), ncol = 3))
# colnames(empty) = c("longitude", "latitude", "address")
# 
# geo = sapply(1:nrow(regions), function(x){
#   charregion <- as.character(regions[x,])
#   geo_result = tryCatch(geocode(charregion, output = 'latlona'), 
#            error = function(e) {empty}) 
#  if (length(geo_result) !=3)
#  {
#    return(empty)
#  }
#    else(return(geo_result))
#   }) 
# 
# geo = t(geo) %>% as.data.frame() %>% mutate(region_1 = regions$region_1)
# 
# save(geo, file="geo.RData")

load(file="geo.RData")

##Now check whether this geocoding worked by seeing if these addresses match the country in the wines dataset

## Step 1: Join those latitude and longitude coordinates to a country ID (function sourced from https://stackoverflow.com/questions/14334970/convert-latitude-and-longitude-coordinates-to-country-name-in-r)

coords2country = function(points)
{  
  countriesSP <- getMap(resolution='low')

  #setting CRS directly to that from rworldmap
  pointsSP <- SpatialPoints(points, proj4string=CRS(proj4string(countriesSP)))  

  # use 'over' to get indices of the Polygons object containing each point 
  indices <- over(pointsSP, countriesSP)

  # return the ISO3 names of each country
  indices$ISO3
}

##Feed this function a dataset called "points" with the long and lat as the 2 columns

geo <- geo %>% filter(is.na(longitude)==FALSE) %>% ##filter unmatched rows
  mutate(longitude=as.numeric(longitude), latitude=as.numeric(latitude))
  
points <- geo %>%  select(longitude, latitude)

geo$countryISO3 <- coords2country(points)

##Step 2: get a country code for the rows in the wines dataset

##Set options for the geonames package
###Set username (I created a profile on the website http://www.geonames.org/)
options(geonamesUsername="bi0260finalproject")
###Set api
options(geonamesHost="api.geonames.org")
###check that connection is working (some samples should pop up)
#source(system.file("tests","testing.R",package="geonames"),echo=TRUE)

##Add a column with the three character country code
wine$countryISO3 <- countrycode(ifelse(wine$country=="England", "United Kingdom", as.character(wine$country)), 'country.name', 'iso3c') 

##Step 3: Join on the lat and long coordinates by region name and check that the countries match

wine_v2 <- left_join(wine, geo, by="region_1")

unmatched <- wine_v2 %>% 
  filter(is.na(longitude)==TRUE | countryISO3.x!=countryISO3.y)

nrow(wine_v2)
nrow(unmatched)

##Fix the latitude and longitude fields to be NA when the countries don't match
wine_v2 <- wine_v2 %>% 
  mutate(longitude=ifelse(countryISO3.x==countryISO3.y, longitude, NA)) %>%
  mutate(latitude=ifelse(countryISO3.x==countryISO3.y, latitude, NA))

##Now for all the unmatched, try to match to coordinates based on province instead

provinces <- as.data.frame(as.character(unique(unmatched$province)))
colnames(provinces) <- c("province")
provinces <- provinces %>% filter(is.na(province)==FALSE & province!="")
nrow(provinces)

##Geocode the regions
##NOTE: Don't run this code unless you need to recreate the table of coordinates. Google have limits for the number of queries. We ran this once and saved it as an R file
# empty = as.data.frame(matrix(rep(NA,3), ncol = 3))
# colnames(empty) = c("longitude", "latitude", "address")
# 
# geo2 = sapply(1:nrow(provinces), function(x){
#   charprovince <- as.character(provinces[x,])
#   geo_result = tryCatch(geocode(charprovince, output = 'latlona'), 
#            error = function(e) {empty}) 
#  if (length(geo_result) !=3)
#  {
#    return(empty)
#  }
#    else(return(geo_result))
#   }) 
# 
# geo2 = t(geo2) %>% as.data.frame() %>% mutate(province = provinces$province)
# 
# save(geo2, file="geo2.RData")

load(file="geo2.RData")

##Now check whether this geocoding worked by seeing if these addresses match the country in the wines dataset

## Step 1: Join those latitude and longitude coordinates to a country ID function 
##Feed the function above into a dataset called "points" with the long and lat as the 2 columns

geo2 <- geo2 %>% filter(is.na(lon)==FALSE) %>% ##filter unmatched rows
  mutate(longitude=as.numeric(lon), latitude=as.numeric(lat))
  
points2 <- geo2 %>%  select(longitude, latitude)

geo2$countryISO3.z <- coords2country(points2)
 head(geo2)

##Step 2: Join on the lat and long coordinates by province name and check that the countries match

wine_v3 <- left_join(wine_v2, geo2, by="province")

unmatched2 <- wine_v3 %>% 
  mutate(longitude=ifelse(is.na(longitude.x)==FALSE,longitude.x, longitude.y )) %>%
  mutate(latitude=ifelse(is.na(latitude.x)==FALSE,latitude.x, latitude.y )) %>%
  filter(is.na(longitude)==TRUE | countryISO3.x!=countryISO3.z)

# nrow(wine_v3)
# nrow(unmatched2)
##We successfully matched 71% 

wine_v3 <- wine_v3 %>% 
  mutate(longitude=ifelse(is.na(longitude.x)==TRUE &
                            countryISO3.x==countryISO3.z,
                          longitude.y, 
                          longitude.x)) %>%
  mutate(latitude=ifelse(is.na(latitude.x)==TRUE &
                            countryISO3.x==countryISO3.z,
                          latitude.y, 
                          latitude.x)) %>%
  select(X, country, designation, points, price, province, region_1, region_2, taster_name, title, variety, winery, year, countryISO3.x, longitude, latitude)

```

### Linking to Climate Data

```{r, warning=FALSE, message=FALSE}
if(!require(rnoaa)){
    install.packages("rnoaa")
    library(rnoaa)
}
# station_data <- ghcnd_stations()

#install pbapply to add a progress bar to the *apply functions and check their progress (very important when code takes many hours to run)
if(!require(pbapply)){
    install.packages("pbapply")
    library(pbapply)
}

##Restrict to just red wines and ones with coordinates (this helped the dataset be a somewhat more manageable size)

redwines <- c("Zinfandel", "Merlot", "Malbec", "Syrah", "Shiraz", "Sangiovese", "Cabernet Sauvignon", "Pinot Noir", "Barbera", "Aglianico", "Tinto Fino", "Tinta de Toro", "Tempranillo Blend", "Tempranillo", "Tannat", "Sangiovese", "Sangiovese Grosso", "Red Blend", "Rhône-style Red Blend", "Primitivo", "Portuguese Red", "Petite Sirah", "Petit Verdot", "Nero d'Avola", "Nerello Mascalese", "Nebbiolo", "Mourvèdre", "Monastrell", "Meritage", "Menc?a", "Malbec-Merlot", "Grenache", "Garnacha", "Gamay", "Corvina, Rondinella, Molinara", "Carmenère", "Cabernet Franc", "Bordeaux-style Red Blend", "Bonarda", "Barbera", "Aglianico")

wine_v4 <- wine_v3 %>% filter(is.na(longitude) == FALSE & variety %in% redwines)

#Obtain the closest weather station id based on latitude and longitude coordinates
##NOTE: Don't run this code unless you need to recreate the tables. We ran this code in batches and then rbind'ed them together. In total, this would take about 5-10 hours depending on your connection speed.
# 
# lat_lon_dfinput <- wine_v4 %>% mutate(id=X) %>% select(id, latitude, longitude)
# 
# 
# stationids <-  pbsapply(1:nrow(lat_lon_dfinput), function(x) {
#   output <- meteo_nearby_stations(lat_lon_df=lat_lon_dfinput[x,], 
#                           station_data = station_data, limit = 1)
#   output[[1]][1]
# }) %>% t() 
# 
# wine_v5 <- wine_v4 %>% 
#   mutate(stationid = as.character(stationids[1,])) %>%
#   select(X, stationid)
# 
# ##inner join back to the wine_v4
# 
# wine_v6 <- inner_join(wine_v4, wine_v5, by="X")
# 
# #getting min and max dates for growing seasons based on hemispheres
# minmonth <- ifelse(wine_v6$latitude>0,"08","02")
# wine_v6$min <- as.Date(paste(wine_v6$year, minmonth, "01", sep="-"), "%Y-%m-%d")
# maxmonth <- ifelse(wine_v6$latitude>0,"10","04")
# maxday <- ifelse(wine_v6$latitude>0, "31", "30")
# wine_v6$max <- as.Date(paste(wine_v6$year, maxmonth, maxday ,sep="-"), "%Y-%m-%d")
# 
# #Obtain the average precipitation and temperature for this date range and weather station combination.
# #precip is in tenths of mm, tempavg is in tenths of degrees C
##NOTE: Don't run this code unless you need to recreate the tables. We ran this code in batches and then rbind'ed them together. In total, this would take about 20 hours- 1.5 days depending on your connection speed.
# 
# empty = matrix(rep(999,2), nrow = 1)
# f = pbsapply(1:nrow(wine_v6), function(x) {
#   tryCatch(meteo_tidy_ghcnd(as.character(wine_v6$stationid[x]), 
#                      keep_flags = FALSE, 
#                      var = c("prcp","tavg"), 
#                      date_min = as.character(wine_v6$min[x]), 
#                      date_max = as.character(wine_v6$max[x])) %>% 
#     # group_by(id) %>%
#     summarize(avgprcp=mean(prcp/10, na.rm=TRUE),
#             avgtemp=mean(tavg/10, na.rm=TRUE)), error = function(e) {empty})
#   }) %>% t() %>% as.data.frame()
# 
# wine_v7 <- wine_v6 %>% 
#   mutate(avgprcp = as.numeric(f$V1), avgtemp = as.numeric(f$V2), year = as.numeric(year))
#
# save(wine_v7, file="wine_v7.RData")

load(file="wine_v7.RData")

```

## Data Analysis

Data analysis for this project included descriptive analyses, data visualizations (including maps), and machine learning.

**Descriptive**

We began with some basic univariate analyses for continuous variables in our dataset:  histograms for wine ratings, price, and years.

```{r, warning=FALSE, message=FALSE}

if(!require(ggthemes)){
    install.packages("ggthemes")
    library(ggthemes)
}

##histogram for wine ratings
p <- wine_v4 %>% 
  ggplot(aes(points)) + 
  geom_histogram(binwidth = 1) + 
  labs(y= "Count", x="Expert Wine Ratings (100 Point Scale)",title="Distribution of Expert Wine Ratings") +
  theme_economist()

# jpeg(filename="hist_ratings.jpg")
p
# dev.off()

summary(wine_v4$points)
sd(wine_v4$points)
```

The distribution of expert wine ratings is approximately normal, with a mean of 89 and a standard deviation of 3. Interestingly, although wine reviews are purportedly on a 100 point scale, scores in our dataset did not drop below 80. As graduate students, we know a little something about this type of "grade inflation".

```{r, warning=FALSE, message=FALSE}
##histogram for wine prices
p <- wine_v4 %>% 
  ggplot(aes(price)) + 
  geom_histogram(binwidth = 0.5) + 
  labs(y= "Count", x="Wine Price ($ per Bottle)",title="Distribution of Wine Prices") +
  theme_economist()

# jpeg(filename="hist_prices.jpg", width = 710, height = 480)
  p + scale_x_continuous(trans = "sqrt", breaks = c(0, 25, 100, 400, 900, 1600, 2500))
# dev.off()

summary(wine_v4$price)
```

The distribution of wine prices is right-skewed, with a median of $40 and a maximum of $3,300.

```{r, warning=FALSE, message=FALSE}
##histogram for years
p <- wine_v4 %>% 
  ggplot(aes(as.numeric(year))) + 
  geom_histogram(binwidth = 1) + 
  labs(y= "Count", x="Vintage Year",title="Distribution of Vintage Years") +
  theme_economist()

# jpeg(filename="hist_vintage.jpg", width = 710, height = 480)
  p + scale_x_continuous(breaks = c(1985, 1990, 1995, 2000, 2005, 2010, 2015))
# dev.off()

summary(as.numeric(wine_v4$year))
```

We restricted to wine reviews from 1985 onward and there were no reviews beyond 2016 in the Kaggle dataset. The majority of reviews were from between 2005 and 2015.

**Data Visualizations**

Data visualizations include average rating by price (correlation plot), average ratings by type of wine (using boxplots), average ratings by country (using maps), average ratings by country and year (using the tile plots in R), average ratings by type of wine and year (using the tile plots in R), and average rating by precipitation and temperature (correlation plots).

We used data visualization to look at predictors of expert ratings in our dataset.

```{r, warning=FALSE, message=FALSE}
##rating by price correlaton
##break this into categories of prices because otherwise the graph is too clumped to see correlations. We can also see from this that price makes a bigger difference in terms of ratings at lower prices and then no difference past $500
p <- wine_v4 %>% 
  filter(is.na(price)==FALSE) %>%
  mutate(pricecat = ifelse(price<100, "< $100 Bottles", ifelse(price<500, "$100-$499.99 Bottles", "\u2265 $500 Bottles"))) %>%
  mutate(pricecat = reorder(pricecat, price, FUN = median)) %>%
  ggplot(aes(price, points)) +
  geom_point()  +
  facet_grid(. ~ pricecat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Wine Price ($ per Bottle)",title="Are Price and Expert Rating Correlated?") +
  theme_economist()

# jpeg(filename="rating_byprice.jpg", width = 710, height = 480)
 p
# dev.off()
```

First, we were interested to know if wine price predicted expert ratings. For the normal price range that the average consumer purchases (0-$100), price was highly predictive of quality, but this association was weaker for more expensive bottles and appeared to be non-existent for bottles over $500. Perhaps when you spend that much on a bottle, your expectations for how it will taste can be too high.

```{r, warning=FALSE, message=FALSE}
##average ratings by wine type
p <- wine_v4 %>% 
  mutate(variety = reorder(variety, -points, FUN = median)) %>%
  ggplot(aes(variety, points)) +
  geom_boxplot()  +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x=" ",title="Do Some Wine Varieties Receive Better Expert Ratings?") +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# jpeg(filename="rating_byvariety.jpg", width = 710, height = 480)
  p
# dev.off()
```

We also examined whether certain varieties of wines (or grapes) were preferred by raters. Sangiovese Grosso tended to be the highest rated and Garnacha the lowest rated, although there was considerable variability in ratings even within a single variety.

```{r, warning=FALSE, message=FALSE}
##average ratings by variety and year
if(!require(RColorBrewer)){
    install.packages("RColorBrewer")
    library(RColorBrewer)
}
dat3 <- wine_v4 %>% 
  filter(is.na(points)==FALSE & is.na(variety)==FALSE & is.na(year)==FALSE) %>%
  group_by(variety, year) %>%
  summarize(meanrating = mean(points), total.count=n()) %>%
  ungroup()

breaks <- seq(80, 94, by=2)
dat3$rating_rescale <- cut(dat3$meanrating,breaks = breaks)
dat3$rating_rescale <- factor(as.character(dat3$rating_rescale),
levels=rev(levels(dat3$rating_rescale)))


p <- dat3 %>% mutate(variety = reorder(variety, meanrating, FUN = median)) %>%
  ggplot(aes(year, variety)) + geom_tile(aes(fill = rating_rescale), colour = "white",size=0.25) + 
  labs(x="Year",y="",title="Have Wine Variety Preferences Changed Over 
Time?") +
  scale_y_discrete(expand=c(0,0)) +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="right", legend.justification = c("left", "top"), legend.title = element_text(size=14)) +
  scale_fill_manual(values=rev(brewer.pal(7,"YlOrRd")),na.value="grey90", name="Mean Expert Rating") +
  guides(fill=guide_legend(ncol=1,bycol=TRUE))

# jpeg(filename="ratings_byvarietyyear.jpg", width = 710, height = 480)
p
# dev.off()
```

We looked at whether there were secular trends in wine preferences. Some wines have always been favorites (Sangiovese Grosso and Nebbiolo) or least liked (Garnacha and Bonarda), while some have fallen out of favor (ex. Corvina, Rondinella, Molinara) or become more popular (Syrah and Shiraz, Grenache, Rhône-style red blend).

**NOTE to Instructors and TAs:  Although the next three blocks of code below ran through and we were able to create maps that were added to the website, when we tried to knit this file the next day, we received an error making the maps. We believe the rworldmap package was updated or for some other reason the identical code we ran the day before no longer works. We prevented it from running below so that it would not impact successfully knitting the rest of the document.**

```{r, warning=FALSE, message=FALSE, eval=FALSE}
##average ratings by country
##Use the rworldmap package
if(!require(rworldmap)){
    install.packages("rworldmap")
    library(rworldmap)
}

###first, aggregate ratings data by country
dat <- wine_v4 %>% 
  filter(is.na(points)==FALSE) %>%
  group_by(countryISO3.x, country) %>%
  summarize(meanrating = mean(points), meanprice = mean(price))

# dat %>% arrange(-meanrating) %>% select(country,countryISO3.x, meanrating)

###then, map these data
sPDF <- joinCountryData2Map(dat,
                            joinCode = "ISO3",
                            nameJoinColumn = "countryISO3.x")

mapDevice() #create world map shaped window


##across the world

# jpeg(filename="rating_bycountry.jpg", width = 710, height = 480)
mapCountryData(sPDF,
               nameColumnToPlot='meanrating',
               mapTitle='Mean Expert Wine Rating by Country')
# dev.off()
```

Some countries are much better known than others for their wines so we were curious if they would be rated better. Interestingly, India, Austria, Morocco, Canada, and South Africa topped the list for highest average ratings, producing fewer but more high quality wines than some of the more notoriously wine producing countries like France, Italy, and Spain.

```{r, warning=FALSE, message=FALSE, eval=FALSE}
##in Europe

# jpeg(filename="rating_byEurope.jpg", width = 710, height = 480)
mapCountryData(sPDF,
               nameColumnToPlot='meanrating',
               mapTitle='Mean Expert Wine Rating by Country in Europe',
               mapRegion="europe")
# dev.off()
```

When we zoomed in on Europe, Austria and Italy were in the lead for highest rated wines. 

```{r, warning=FALSE, message=FALSE, eval=FALSE}

##Mean Prices by country (not shown on website)
##across the world
mapCountryData(sPDF,
               nameColumnToPlot='meanprice',
               mapTitle='Mean Wine Price by Country')

##in Europe
mapCountryData(sPDF,
               nameColumnToPlot='meanprice',
               mapTitle='Mean Wine Price by Country in Europe',
               mapRegion="europe")
```

```{r, warning=FALSE, message=FALSE}
##average ratings by country and year
dat2 <- wine_v4 %>% 
  filter(is.na(points)==FALSE & is.na(year)==FALSE) %>%
  group_by(country, year) %>%
  summarize(meanrating = mean(points), total.count=n()) %>%
  ungroup()

breaks <- seq(80, 94, by=2)
dat2$rating_rescale <- cut(dat2$meanrating,breaks = breaks)
dat2$rating_rescale <- factor(as.character(dat2$rating_rescale),
levels=rev(levels(dat2$rating_rescale)))


p <- dat2 %>% mutate(country = reorder(country, total.count, FUN = max)) %>%
  ggplot(aes(year, country)) + geom_tile(aes(fill = rating_rescale), colour = "white",size=0.25) + 
  labs(x="Year",y="",title="Have Country-Specific Wine Preferences Changed Over 
Time?") +
  scale_y_discrete(expand=c(0,0)) +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="right", legend.justification = c("left", "top"), legend.title = element_text(size=14)) +
  scale_fill_manual(values=rev(brewer.pal(7,"YlOrRd")),na.value="grey90", name="Mean Expert Rating") +
  guides(fill=guide_legend(ncol=1,bycol=TRUE))

# jpeg(filename="ratings_bycountryyear.jpg", width = 710, height = 480)
p
# dev.off()
```

We were also curious if there were trends over time in these ratings. Although there weren't reviews for every country and year combination, we did notice that countries like South Africa and Australia have really improved over time. 

```{r, warning=FALSE, message=FALSE}
##Correlation between average precipitation and wine rating
p <- wine_v7 %>% 
  filter(avgprcp!=999) %>%
  mutate(raincat = ifelse(avgprcp<10, "< 10 mm", "\u2265 10 mm")) %>%
  mutate(raincat = reorder(raincat, avgprcp, FUN = median)) %>%
  ggplot(aes(avgprcp, points)) +
  geom_point()  +
  facet_grid(. ~ raincat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Average Daily Rainfall During Harvesting Season (mm)",title="Are Rainfall and Expert Rating Correlated?") +
  theme_economist()

 # jpeg(filename="rating_byrain.jpg", width = 710, height = 480)
 p
 # dev.off()
```
 
Usually you hear that drought is associated with better quality, but in our data set, we find that higher rainfall during the harvesting season is associated with better quality.  It is possible that the relationship is more complex and may vary across wine type.  The machine learning algorithm used (detailed in the next section) may better handle these types of relationships.
 
```{r, warning=FALSE, message=FALSE}
 ##Correlation between average temperature and wine rating
p <- wine_v7 %>% 
  filter(avgtemp!=999) %>%
  mutate(tempcat = ifelse(avgtemp<13, "< 13 (degrees C)", "\u2265 13 (degrees C)")) %>%
  mutate(tempcat = reorder(tempcat, avgtemp, FUN = median)) %>%
  ggplot(aes(avgtemp, points)) +
  geom_point()  +
  facet_grid(. ~ tempcat, scales="free_x") +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Expert Wine Ratings (100 Point Scale)", x="Average Temperature During Harvesting Season (degrees C)",title="Are Temperature and Expert Rating Correlated?") +
  theme_economist()

 # jpeg(filename="rating_bytemp.jpg", width = 710, height = 480)
 p
 # dev.off()
```

The majority of wines are harvested in the 13-25 $^\circ$ Celsius range;  however, some are successfully harvested in colder climates.  It seems that colder climates are associated with better wine quality.

**Machine Learning**

We predicted wine ratings based on the other characteristics available in our dataset (including price, longitude, latitude, wine variety, year, precipitation, and temperature) using random forests.

```{r}
if(!require(randomForest)){
    install.packages("randomForest")
    library(randomForest)
}

if(!require(caret)){
    install.packages("caret")
    library(caret)
}

set.seed(658)

##Create a dataset with no missing information
##also, reset wine variety as factor because it current recognizes >700 levels of variety from the original dataset when there are now only 35 in our final dataset
wine_v7_ml <- wine_v7 %>% 
  filter(avgtemp!=999 & 
           avgprcp!=999 & 
           is.na(points)==FALSE & 
           is.na(price)==FALSE & 
           is.na(longitude)==FALSE &
           is.na(variety)==FALSE &
           is.na(year)==FALSE) %>%
  mutate(variety = as.factor(as.character(variety)))

##rows in final dataset
nrow(wine_v7_ml)
         
##Step 1: run a basic random forest to get started

fit <- randomForest(points ~ price + longitude + latitude  + variety +
                      year + avgtemp + avgprcp,
                      data=wine_v7_ml, 
                      importance=TRUE, 
                      ntree=500)

fit

##Step 2: decide how many trees need to be run
##plot fit to see how it improves with each tree
plot(fit)

##identify the minimum mse by number of trees
which.min(fit$mse)
  ##although we don't get that much return by fitting more trees, the minimum mse is at 488 trees so we will fit that many in analyses below.

##Step 3: tune the "mtry" variable which defines how many features are randomly selected at each split. By default, it uses 2 (we can see this in the output of "fit" and the default for numeric responses is max(floor(ncol(x)/3))
##NOTE: This takes about 20 minutes so don't need to re-run

# ctrl <- trainControl(method="repeatedcv", 
#                      number=10, 
#                      repeats=10,
#                      verboseIter = TRUE)
# 
# grid_rf <-expand.grid(.mtry = c(1, 2, 4, 6))
# 
# m_rf <- train(points ~ price + longitude + latitude  + variety +
#                       year + avgtemp + avgprcp,
#               data=wine_v7_ml, method="rf", 
#               metric="RMSE", trcontrol=ctrl, 
#               tuneGrid=grid_rf)
# 
# m_rf

#Re-fit the random forest with mtry=6
fit <- randomForest(points ~ price + longitude + latitude  + variety +
                      year + avgtemp + avgprcp,
                    data=wine_v7_ml, 
                    importance=TRUE, 
                    ntree=500,
                    mtry=6)

fit
```

We used 10-fold cross-validation to tune the parameter that defines how many features are randomly selected for splitting at each tree node. Random forests are relatively robust to overfitting and cross-validation confirmed that measures of prediction error, including root mean squared error (RMSE) and R-squared, were similar when the random forest was trained and measures of error estimated using our full dataset.  We were able to explain a little over 50% of the variation in expert ratings based on our model.

```{r, warning=FALSE, message=FALSE}
##Plot the actual vs predicted values (a visual way of looking at the error)
pred.forest <- predict(fit,wine_v7_ml)

p <- wine_v7_ml %>% 
  ggplot(aes(points, pred.forest)) +
  geom_point()  +
  geom_smooth(method = "lm", size = 1.5) +
  labs(y= "Predicted Ratings", x="Actual Ratings",title="Predicted vs. Actual Ratings Using Random Forest") +
  theme_economist()

 # jpeg(filename="predvsactual.jpg", width = 426, height = 288)
 p
 # dev.off()
 
 fit
```

Predicted and actual ratings were fairly highly correlated across the range of ratings.

```{r, warning=FALSE, message=FALSE}

##plot the variable importance
# jpeg(filename="predvsactual.jpg", width = 710, height = 480)
varImpPlot(fit,
           sort = T,
           main="Variable Importance")
```

Using a few different measures of variable importance, price was the standout winner of most important variable for predicting ratings. Variety of wine was also an important predictor, with the other variables playing more minor roles.

**Conclusions**

This analysis helped us answer the following research questions:

1. What factors are most significantly associated with wine quality?

**We found that price and, to a lesser extent, wine variety were the strongest predictors of wine quality.**

2. Are certain climate trends (extreme temperatures and considerable rainfall) during the wine’s harvesting season important determinants of wine quality?◦

**Climate trends were not strong predictors of wine quality, but higher precipitation and lower temperatures during the harvesting season were associated with better wine ratings. When a sommelier tells you how the drought in this region led to a better wine, you may want to ask him/her for some data to back that up!**

3. Do you "get what you pay for" when you buy an expensive bottle of wine?

**Price is the strongest predictor of wine rating, but for very expensive bottles (>$500) price does not necessarily lead to higher quality. So you may not want to splurge on that $3,000 bottle!**

4. Can a machine learning algorithm be built that could predict wine quality as effectively as expert tasters/raters?   

**We don't think machines will be replacing wine tasters any time soon, but we were able to explain a little over half the variation in wine ratings through our random forest prediction.**  

Go grab that Sangiovesse Grosso and enjoy!

